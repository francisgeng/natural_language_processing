{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW2A: Alignment with IBM Model 1","metadata":{}},{"cell_type":"code","source":"# Some of the functions below require an older version of torchtext than the default one Kaggle gives you.\n# IMPORTANT: Make sure that Internet is turned on!!! (Notebook options in the bar on the right)\n# IMPORTANT: If you're not already using Kaggle, we STRONGLY recommend you switch to Kaggle for hw1b in particular,\n# because copying our notebook will pin you to a Python version that lets you install the right version of torchtext.\n# On Colab you will have to downgrade your Python to e.g., 3.7 to do the below pip install, which is a pain to do.\n!pip install torchtext==0.8.1\nexit()","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:28:35.652916Z","iopub.execute_input":"2023-09-20T06:28:35.653415Z","iopub.status.idle":"2023-09-20T06:29:51.798680Z","shell.execute_reply.started":"2023-09-20T06:28:35.653375Z","shell.execute_reply":"2023-09-20T06:29:51.795854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport math\nimport matplotlib.pyplot as plt # graphs and figures\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport string\nfrom collections import Counter\nfrom itertools import product\nimport tqdm.notebook\nimport pickle\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:30:06.186353Z","iopub.execute_input":"2023-09-20T06:30:06.186800Z","iopub.status.idle":"2023-09-20T06:30:06.195764Z","shell.execute_reply.started":"2023-09-20T06:30:06.186762Z","shell.execute_reply":"2023-09-20T06:30:06.194660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data\n\nWe'll start out by using a toy dataset. Please see [these slides](https://cal-cs288.github.io/sp20/slides/cs288_sp20_05_statistical_translation_4up.pdf) for a more complete coverage of IBM Model 1, and feel free to check out Philipp Koehn's book _Statistical Machine Translation_.","metadata":{}},{"cell_type":"code","source":"aligned_data = [\n    ([\"das\", \"haus\"], [\"the\", \"house\"]),\n    ([\"das\", \"buch\"], [\"the\", \"book\"]),\n    ([\"ein\", \"buch\"], [\"a\", \"book\"]),\n]","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:30:14.069265Z","iopub.execute_input":"2023-09-20T06:30:14.069857Z","iopub.status.idle":"2023-09-20T06:30:14.077454Z","shell.execute_reply.started":"2023-09-20T06:30:14.069807Z","shell.execute_reply":"2023-09-20T06:30:14.076189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Alignment Model\n\nFill in the code for IBM Model 1 below. A correct implementation should achieve perplexity 4096 on the first iteration and perplexity around 70 by the tenth iteration, for the toy dataset above. Note that we'll be grading you only on the generated `self.translation_probabilities`, so the probability and perplexity functions only exist for you to check the correctness of your own implementation. You may wish to comment them out during implementation and check that `self.translation_probabilities` looks reasonable instead.","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nclass IBMModel1:\n    def __init__(self, data, num_iterations=10, epsilon=1.0, compute_perplexity=True):\n        self.data = data # aligned corpus as shown above\n        self.num_iterations = num_iterations # iterations of expectation-maximization\n        self.epsilon = epsilon\n        self.compute_perplexity = compute_perplexity\n\n        # Preprocess bitext data:\n        self.source_words, self.target_words = set(), set()\n        for (source,target) in self.data:\n            self.source_words.update(source)\n            self.target_words.update(target)\n\n        # Initialize uniform probabilities:\n        \n#         self.translation_probs = {}\n#         for (source,target) in self.data:\n#             for s in source:\n#                 for t in target:\n#                     self.translation_probs[(s,t)] = 1/len(source)\n# #         self.translation_probs = defaultdict(float)\n\n        self.translation_probs = {(s,t): 1.0/len(self.target_words)\n                                  for s,t in product(self.source_words, self.target_words)}\n            \n        \n\n    def e_step(self):\n        # YOUR SOLUTION HERE\n        # - Iterate over paired sentences in the data and compute:\n        # - (1) counts, the number of times a source word is translated into a target word,\n        #       weighted by alignment probabilities\n        # - (2) total, the sum of counts over all possible target words\n        # See slide 32 for more information: https://cal-cs288.github.io/sp20/slides/cs288_sp20_05_statistical_translation_4up.pdf\n        # BEGIN SOLUTION\n        counts = defaultdict(float)\n        total = defaultdict(float)\n        s_total = defaultdict(float)\n        for (source, target) in self.data:\n            for t in target: \n                s_total[t] = 0\n                for s in source:\n                    s_total[t] += self.translation_probs[(s,t)]\n            for t in target:\n                for s in source:\n                    c = self.translation_probs[(s,t)] / s_total[t]\n                    counts[(s,t)] += c\n                    total[s] += c\n\n        return counts, total\n        # END SOLUTION\n\n    def m_step(self, counts, total):\n        # YOUR SOLUTION HERE\n        # - Update self.translation_probs using counts and total\n        # BEGIN SOLUTION\n        for (source, target) in self.data:\n            for s in source:\n                for t in target:\n                    self.translation_probs[(s,t)] = counts[(s,t)]/total[s]\n        # END SOLUTION\n\n    def train(self):\n        # Run EM for self.num_iterations:\n        for idx in tqdm.tqdm(range(self.num_iterations)):\n            if self.compute_perplexity:\n                print(\"Iteration: {} | Perplexity: {}\".format(idx, self.perplexity()))\n            counts, total = self.e_step()\n            self.m_step(counts, total)\n        if self.compute_perplexity:\n            print(\"Iteration: {} | Perplexity: {}\".format(self.num_iterations, self.perplexity()))\n    def probability(self, source, target):\n        # YOUR SOLUTION HERE\n        # - Use the normalization trick from lecture to efficiently compute probabilities\n        # - We'll use self.epsilon here, which is defined in the initialization\n        # BEGIN SOLUTION\n#         probs = 1.0\n#         for (source, target) in self.data:\n#             for t in target:\n#                 probs *= sum([self.translation_probs[(s,t)] for s in source])\n#         probs *= self.epsilon /((len(source)+1)^(len(target)))\n#         return probs\n        probs = 1.0\n        for t in target: \n            probs *= sum([self.translation_probs[(s,t)] for s in source])\n        probs *= 1 /((len(source))**(len(target)))\n        return probs\n        # END SOLUTION\n\n    def perplexity(self):\n        # YOUR SOLUTION HERE\n        # - Iterate over each pair of sentences in the dataset\n        # - Call self.probability and compute a sum in log space\n        # - Feel free to comment this out while testing your initial model\n        # BEGIN SOLUTION\n        log_prob = 0\n        for (source, target) in self.data:\n            log_prob += math.log2(self.probability(source, target))\n        return 2**(-log_prob)\n        # END SOLUTION\n\n    def get_alignment(self, source, target):\n        # YOUR SOLUTION HERE\n        # - Find the best word alignment for a source, target pair\n        # - Output a list of [(source_idx, target_idx)]\n        #   For example: ([\"ein\", \"buch\"], [\"a\", \"book\"])\n        #   should have an alignment [(0,0), (1,1)]\n        # BEGIN SOLUTION\n        alignments = []\n        for i, s in enumerate(source):\n            best_prob = 0\n            best_j = -1\n            for j, t in enumerate(target):\n                if self.translation_probs[(s, t)] > best_prob:\n                    best_prob = self.translation_probs[(s, t)]\n                    best_j = j\n            alignments.append((i, best_j))\n        return alignments\n        # END SOLUTION\n\nibm = IBMModel1(aligned_data)\nibm.train()\nassert(ibm.get_alignment([\"ein\", \"buch\"], [\"a\", \"book\"]) == [(0, 0), (1, 1)])\nwith open(\"example_alignments.pkl\", \"wb\") as outfile:\n    pickle.dump(ibm.translation_probs, outfile, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:30:16.313014Z","iopub.execute_input":"2023-09-20T06:30:16.313978Z","iopub.status.idle":"2023-09-20T06:30:16.364219Z","shell.execute_reply.started":"2023-09-20T06:30:16.313922Z","shell.execute_reply":"2023-09-20T06:30:16.362659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Visualization and Analysis\n\nWrite code to visualize alignments and rerun the IBM model on a (very slightly larger) toy dataset:","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_alignment(alignment, source_words, target_words):\n    # Initialize a 2D matrix with zeros\n    matrix = np.zeros((len(source_words), len(target_words)))\n\n    # Fill in the matrix based on the alignment\n    for source_idx, target_idx in alignment:\n        matrix[source_idx][target_idx] = 1\n\n    # Use Matplotlib to visualize the matrix\n    plt.imshow(matrix, cmap='gray_r', interpolation='nearest')\n\n    # Label the axes\n    plt.xticks(np.arange(len(target_words)), target_words, rotation=45)\n    plt.yticks(np.arange(len(source_words)), source_words)\n\n    # Add a color bar\n    plt.colorbar(label='Alignment')\n\n    # Show the plot\n    plt.show()\n\n\naligned_data = [\n    (['klein', 'ist', 'das', 'haus'], ['the', 'house', 'is', 'small']),\n    (['das', 'haus', 'ist', 'ja', 'gro√ü'], ['the', 'house', 'is', 'big']),\n    (['das', 'buch', 'ist', 'ja', 'klein'], ['the', 'book', 'is', 'small']),\n    (['das', 'haus'], ['the', 'house']),\n    (['das', 'buch'], ['the', 'book']),\n    (['ein', 'buch'], ['a', 'book'])\n]\nibm = IBMModel1(aligned_data)\nibm.train()\nalignment = ibm.get_alignment(['klein', 'ist', 'das', 'haus'], ['the', 'house', 'is', 'small'])\nvisualize_alignment(alignment, ['klein', 'ist', 'das', 'haus'], ['the', 'house', 'is', 'small'])","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:30:18.708942Z","iopub.execute_input":"2023-09-20T06:30:18.710605Z","iopub.status.idle":"2023-09-20T06:30:19.067580Z","shell.execute_reply.started":"2023-09-20T06:30:18.710498Z","shell.execute_reply":"2023-09-20T06:30:19.066263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll now run the IBM model on a significantly larger dataset to showcase its failure modes:\n","metadata":{}},{"cell_type":"code","source":"import sentencepiece\nimport torchtext\nfrom torchtext.datasets import Multi30k","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:30:31.723280Z","iopub.execute_input":"2023-09-20T06:30:31.723849Z","iopub.status.idle":"2023-09-20T06:30:32.437662Z","shell.execute_reply.started":"2023-09-20T06:30:31.723800Z","shell.execute_reply":"2023-09-20T06:30:32.436126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Multi30k.urls = [\n    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\",\n    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\",\n    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n]\n\nextensions = [\".de\", \".en\"]\nsource_field = torchtext.data.Field(tokenize=lambda x: x)\ntarget_field = torchtext.data.Field(tokenize=lambda x: x)\ntraining_data, validation_data, test_data = torchtext.datasets.Multi30k.splits(\n    extensions, [source_field, target_field], root=\"/kaggle/working/\", test=\"test\")","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:30:33.324652Z","iopub.execute_input":"2023-09-20T06:30:33.325242Z","iopub.status.idle":"2023-09-20T06:30:34.357075Z","shell.execute_reply.started":"2023-09-20T06:30:33.325195Z","shell.execute_reply":"2023-09-20T06:30:34.355775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(sentence):\n    sentence = sentence.translate(str.maketrans('', '', string.punctuation)) # strip punctuation\n    return sentence.strip().lower().split()\n\naligned_data = []\nfor example in training_data[:1000]:\n    source = preprocess(example.src)\n    target = preprocess(example.trg)\n    aligned_data.append((source, target))\n\nibm = IBMModel1(aligned_data, compute_perplexity=False)\nibm.train()\nwith open(\"multi30k_alignments.pkl\", \"wb\") as outfile:\n    pickle.dump(ibm.translation_probs, outfile, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:30:35.642585Z","iopub.execute_input":"2023-09-20T06:30:35.643011Z","iopub.status.idle":"2023-09-20T06:30:50.840661Z","shell.execute_reply.started":"2023-09-20T06:30:35.642969Z","shell.execute_reply":"2023-09-20T06:30:50.838379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Making sure the model learned something:\nexamples = [\n    (\"hund\", \"dog\"),\n    (\"hund\", \"cat\"),\n    (\"ein\", \"a\"),\n    (\"ein\", \"the\"),\n    (\"frau\", \"woman\"),\n    (\"frau\", \"man\"),\n]\n\n\nfor example in examples:\n    print(str(example) + \": \" + str(ibm.translation_probs[example]))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:30:50.843172Z","iopub.execute_input":"2023-09-20T06:30:50.843697Z","iopub.status.idle":"2023-09-20T06:30:50.854729Z","shell.execute_reply.started":"2023-09-20T06:30:50.843653Z","shell.execute_reply":"2023-09-20T06:30:50.853171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"source = ['die', 'frau', 'hat', 'einen', 'kleinen', 'hund']\ntarget = ['the', 'woman', 'has', 'a', 'small','dog']\nalignment = ibm.get_alignment(source, target)\nvisualize_alignment(alignment, source, target)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:30:55.457553Z","iopub.execute_input":"2023-09-20T06:30:55.458392Z","iopub.status.idle":"2023-09-20T06:30:55.762991Z","shell.execute_reply.started":"2023-09-20T06:30:55.458309Z","shell.execute_reply":"2023-09-20T06:30:55.761611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"source = ['hat', 'haben', 'see']\ntarget = ['has','have', 'ocean']\nalignment = ibm.get_alignment(source, target)\nvisualize_alignment(alignment, source, target)","metadata":{"execution":{"iopub.status.busy":"2023-09-20T06:33:34.667075Z","iopub.execute_input":"2023-09-20T06:33:34.667720Z","iopub.status.idle":"2023-09-20T06:33:34.921606Z","shell.execute_reply.started":"2023-09-20T06:33:34.667653Z","shell.execute_reply":"2023-09-20T06:33:34.920519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From this larger dataset: find at least one sentence where the IBM alignment model performs reasonably well, and find another one where it fails catastrophically, and include alignment visualizations for both examples in your report. You may want to consult a [German-English dictionary](https://www.collinsdictionary.com/us/dictionary/english-german) for this part of the problem. Provide a brief explanation for why the alignment model did poorly on the failure case.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}