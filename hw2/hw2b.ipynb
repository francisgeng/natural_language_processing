{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW2B: Neural Machine Translation","metadata":{}},{"cell_type":"markdown","source":"In this project, you will build a neural machine translation system using modern techniques for sequence-to-sequence modeling. You will first implement a baseline encoder-decoder architecture, then improve upon the baseline by adding an attention mechanism. The end result will be a fully functional translation system capable of translating simple German sentences into English.","metadata":{}},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"markdown","source":"First we install and import the required dependencies. These include:\n* `torch` for modeling and training\n* `torchtext` for data collection\n* `sentencepiece` for subword tokenization\n* `sacrebleu` for BLEU score evaluation","metadata":{}},{"cell_type":"code","source":"# Some of the functions below require an older version of torchtext than the default one Kaggle gives you.\n# IMPORTANT: Make sure that Internet is turned on!!! (Notebook options in the bar on the right)\n# IMPORTANT: If you're not already using Kaggle, we STRONGLY recommend you switch to Kaggle for hw1b in particular,\n# because copying our notebook will pin you to a Python version that lets you install the right version of torchtext.\n# On Colab you will have to downgrade your Python to e.g., 3.7 to do the below pip install, which is a pain to do.\n!pip install torchtext==0.8.1\nexit()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:28:55.873909Z","iopub.execute_input":"2023-09-29T04:28:55.874287Z","iopub.status.idle":"2023-09-29T04:30:00.428492Z","shell.execute_reply.started":"2023-09-29T04:28:55.874214Z","shell.execute_reply":"2023-09-29T04:30:00.426962Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torchtext==0.8.1\n  Downloading torchtext-0.8.1-cp37-cp37m-manylinux1_x86_64.whl (7.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torch==1.7.1\n  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.8/776.8 MB\u001b[0m \u001b[31m953.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.8.1) (4.64.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchtext==0.8.1) (1.21.6)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.8.1) (2.28.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.7.1->torchtext==0.8.1) (4.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.1) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.1) (1.26.14)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.1) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.8.1) (2022.12.7)\nInstalling collected packages: torch, torchtext\n  Attempting uninstall: torch\n    Found existing installation: torch 1.11.0\n    Uninstalling torch-1.11.0:\n      Successfully uninstalled torch-1.11.0\n  Attempting uninstall: torchtext\n    Found existing installation: torchtext 0.12.0\n    Uninstalling torchtext-0.12.0:\n      Successfully uninstalled torchtext-0.12.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchmetrics 0.11.0 requires torch>=1.8.1, but you have torch 1.7.1 which is incompatible.\npytorch-lightning 1.9.0 requires torch>=1.10.0, but you have torch 1.7.1 which is incompatible.\nfairscale 0.4.6 requires torch>=1.8.0, but you have torch 1.7.1 which is incompatible.\nallennlp 2.10.1 requires torch<1.13.0,>=1.10.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-1.7.1 torchtext-0.8.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"%%capture\n!pip install --upgrade sacrebleu sentencepiece\n\n# Standard library imports\nimport json\nimport math\nimport random\nimport pdb\n\n# Third party imports\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sacrebleu\nimport sentencepiece\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchtext\nimport tqdm.notebook","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:30:18.487238Z","iopub.execute_input":"2023-09-29T04:30:18.487688Z","iopub.status.idle":"2023-09-29T04:30:28.261584Z","shell.execute_reply.started":"2023-09-29T04:30:18.487604Z","shell.execute_reply":"2023-09-29T04:30:28.260544Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Before proceeding, let's verify that we're connected to a GPU runtime and that `torch` can detect the GPU.\nWe'll define a variable `device` here to use throughout the code so that we can easily change to run on CPU for debugging.","metadata":{}},{"cell_type":"code","source":"assert torch.cuda.is_available()\ndevice = torch.device(\"cuda\")\nprint(\"Using device:\", device)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:31:27.192320Z","iopub.execute_input":"2023-09-29T04:31:27.192896Z","iopub.status.idle":"2023-09-29T04:31:27.312992Z","shell.execute_reply.started":"2023-09-29T04:31:27.192836Z","shell.execute_reply":"2023-09-29T04:31:27.311711Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"markdown","source":"The data for this assignment comes from the [Multi30K dataset](https://arxiv.org/abs/1605.00459), which contains English and German captions for images from Flickr. We can download and unpack it using `torchtext`. We use the Multi30K dataset because it is simpler than standard translation benchmark datasets and allows for models to be trained and evaluated in a matter of minutes rather than days.\n\nWe will be translating from German to English in this assignment, but the same techniques apply equally well to any language pair.\n\n","metadata":{}},{"cell_type":"code","source":"import torchtext\nfrom torchtext.datasets import Multi30k\n\nMulti30k.urls = [\n    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/training.tar.gz\",\n    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/validation.tar.gz\",\n    \"https://raw.githubusercontent.com/neychev/small_DL_repo/master/datasets/Multi30k/mmt16_task1_test.tar.gz\"\n]\n\nextensions = [\".de\", \".en\"]\nsource_field = torchtext.data.Field(tokenize=lambda x: x)\ntarget_field = torchtext.data.Field(tokenize=lambda x: x)\ntraining_data, validation_data, test_data = torchtext.datasets.Multi30k.splits(\n    extensions, [source_field, target_field], root=\"/kaggle/working/\", test=\"test\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:31:29.529438Z","iopub.execute_input":"2023-09-29T04:31:29.530014Z","iopub.status.idle":"2023-09-29T04:31:30.349127Z","shell.execute_reply.started":"2023-09-29T04:31:29.529982Z","shell.execute_reply":"2023-09-29T04:31:30.348125Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"downloading training.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 53.6MB/s]\n","output_type":"stream"},{"name":"stdout","text":"downloading validation.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 9.14MB/s]","output_type":"stream"},{"name":"stdout","text":"downloading mmt16_task1_test.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"\nmmt16_task1_test.tar.gz: 100%|██████████| 67.1k/67.1k [00:00<00:00, 15.9MB/s]\n/opt/conda/lib/python3.7/site-packages/torchtext/data/example.py:78: UserWarning: Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n  warnings.warn('Example class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.', UserWarning)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that we have the data, let's see how large each split is and look at a few examples.\n","metadata":{}},{"cell_type":"code","source":"print(\"Number of training examples:\", len(training_data))\nprint(\"Number of validation examples:\", len(validation_data))\nprint(\"Number of test examples:\", len(test_data))\nprint()\n\nfor example in training_data[:10]:\n  print(example.src)\n  print(example.trg)\n  print()","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:31:30.890362Z","iopub.execute_input":"2023-09-29T04:31:30.890731Z","iopub.status.idle":"2023-09-29T04:31:30.897282Z","shell.execute_reply.started":"2023-09-29T04:31:30.890681Z","shell.execute_reply":"2023-09-29T04:31:30.896341Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Number of training examples: 29000\nNumber of validation examples: 1014\nNumber of test examples: 1000\n\nZwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\nTwo young, White males are outside near many bushes.\n\nMehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\nSeveral men in hard hats are operating a giant pulley system.\n\nEin kleines Mädchen klettert in ein Spielhaus aus Holz.\nA little girl climbing into a wooden playhouse.\n\nEin Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\nA man in a blue shirt is standing on a ladder cleaning a window.\n\nZwei Männer stehen am Herd und bereiten Essen zu.\nTwo men are at the stove preparing food.\n\nEin Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\nA man in green holds a guitar while the other man observes his shirt.\n\nEin Mann lächelt einen ausgestopften Löwen an.\nA man is smiling at a stuffed lion\n\nEin schickes Mädchen spricht mit dem Handy während sie langsam die Straße entlangschwebt.\nA trendy girl talking on her cellphone while gliding slowly down the street.\n\nEine Frau mit einer großen Geldbörse geht an einem Tor vorbei.\nA woman with a large purse is walking by a gate.\n\nJungen tanzen mitten in der Nacht auf Pfosten.\nBoys dancing on poles in the middle of the night.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Vocabulary","metadata":{}},{"cell_type":"markdown","source":"We can use `sentencepiece` to create a joint German-English subword vocabulary from the training corpus. Because the number of training examples is small, we choose a smaller vocabulary size than would be used for large-scale NMT.","metadata":{}},{"cell_type":"code","source":"args = {\n    \"pad_id\": 0,\n    \"bos_id\": 1,\n    \"eos_id\": 2,\n    \"unk_id\": 3,\n    \"input\": \"multi30k/train.de,multi30k/train.en\",\n    \"vocab_size\": 8000,\n    \"model_prefix\": \"multi30k\",\n}\ncombined_args = \" \".join(\n    \"--{}={}\".format(key, value) for key, value in args.items())\nsentencepiece.SentencePieceTrainer.Train(combined_args)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:31:32.435235Z","iopub.execute_input":"2023-09-29T04:31:32.435589Z","iopub.status.idle":"2023-09-29T04:31:38.302254Z","shell.execute_reply.started":"2023-09-29T04:31:32.435560Z","shell.execute_reply":"2023-09-29T04:31:38.300244Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"sentencepiece_trainer.cc(177) LOG(INFO) Running command: --pad_id=0 --bos_id=1 --eos_id=2 --unk_id=3 --input=multi30k/train.de,multi30k/train.en --vocab_size=8000 --model_prefix=multi30k\nsentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \ntrainer_spec {\n  input: multi30k/train.de\n  input: multi30k/train.en\n  input_format: \n  model_prefix: multi30k\n  model_type: UNIGRAM\n  vocab_size: 8000\n  self_test_sample_size: 0\n  character_coverage: 0.9995\n  input_sentence_size: 0\n  shuffle_input_sentence: 1\n  seed_sentencepiece_size: 1000000\n  shrinking_factor: 0.75\n  max_sentence_length: 4192\n  num_threads: 16\n  num_sub_iterations: 2\n  max_sentencepiece_length: 16\n  split_by_unicode_script: 1\n  split_by_number: 1\n  split_by_whitespace: 1\n  split_digits: 0\n  pretokenization_delimiter: \n  treat_whitespace_as_suffix: 0\n  allow_whitespace_only_pieces: 0\n  required_chars: \n  byte_fallback: 0\n  vocabulary_output_piece_score: 1\n  train_extremely_large_corpus: 0\n  hard_vocab_limit: 1\n  use_all_vocab: 0\n  unk_id: 3\n  bos_id: 1\n  eos_id: 2\n  pad_id: 0\n  unk_piece: <unk>\n  bos_piece: <s>\n  eos_piece: </s>\n  pad_piece: <pad>\n  unk_surface:  ⁇ \n  enable_differential_privacy: 0\n  differential_privacy_noise_level: 0\n  differential_privacy_clipping_threshold: 0\n}\nnormalizer_spec {\n  name: nmt_nfkc\n  add_dummy_prefix: 1\n  remove_extra_whitespaces: 1\n  escape_whitespaces: 1\n  normalization_rule_tsv: \n}\ndenormalizer_spec {}\ntrainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\ntrainer_interface.cc(183) LOG(INFO) Loading corpus: multi30k/train.de\ntrainer_interface.cc(183) LOG(INFO) Loading corpus: multi30k/train.en\ntrainer_interface.cc(407) LOG(INFO) Loaded all 58000 sentences\ntrainer_interface.cc(423) LOG(INFO) Adding meta_piece: <pad>\ntrainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\ntrainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\ntrainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\ntrainer_interface.cc(428) LOG(INFO) Normalizing sentences...\ntrainer_interface.cc(537) LOG(INFO) all chars count=3877107\ntrainer_interface.cc(548) LOG(INFO) Done: 99.9537% characters are covered.\ntrainer_interface.cc(558) LOG(INFO) Alphabet size=60\ntrainer_interface.cc(559) LOG(INFO) Final character coverage=0.999537\ntrainer_interface.cc(591) LOG(INFO) Done! preprocessed 58000 sentences.\nunigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\nunigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=2139779\nunigram_model_trainer.cc(274) LOG(INFO) Initialized 81782 seed sentencepieces\ntrainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 58000\ntrainer_interface.cc(608) LOG(INFO) Done! 39388\nunigram_model_trainer.cc(564) LOG(INFO) Using 39388 sentences for EM training\nunigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=24756 obj=10.2757 num_tokens=79134 num_tokens/piece=3.19656\nunigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=20873 obj=8.08679 num_tokens=79533 num_tokens/piece=3.81033\nunigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=15641 obj=8.07036 num_tokens=85721 num_tokens/piece=5.48053\nunigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=15630 obj=8.04388 num_tokens=85743 num_tokens/piece=5.4858\nunigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=11722 obj=8.16114 num_tokens=95990 num_tokens/piece=8.18888\nunigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=11722 obj=8.13206 num_tokens=95994 num_tokens/piece=8.18922\nunigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=8800 obj=8.30125 num_tokens=107803 num_tokens/piece=12.2503\nunigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=8800 obj=8.27055 num_tokens=107797 num_tokens/piece=12.2497\ntrainer_interface.cc(686) LOG(INFO) Saving model: multi30k.model\ntrainer_interface.cc(698) LOG(INFO) Saving vocabs: multi30k.vocab\n","output_type":"stream"}]},{"cell_type":"markdown","source":"This creates two files: `multi30k.model` and `multi30k.vocab`. The first is a binary file containing the relevant data for the vocabulary. The second is a human-readable listing of each subword and its associated score.","metadata":{}},{"cell_type":"markdown","source":"We can preview the contents of the vocabulary by looking at the first few rows from the human-readable file.","metadata":{}},{"cell_type":"code","source":"!head -n 30 multi30k.vocab","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:31:40.992156Z","iopub.execute_input":"2023-09-29T04:31:40.993297Z","iopub.status.idle":"2023-09-29T04:31:42.003082Z","shell.execute_reply.started":"2023-09-29T04:31:40.993261Z","shell.execute_reply":"2023-09-29T04:31:42.001909Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<pad>\t0\n<s>\t0\n</s>\t0\n<unk>\t0\n.\t-2.72718\n▁a\t-3.21357\n▁in\t-3.43973\nm\t-3.78503\n▁eine\t-3.82141\n▁A\t-3.86856\ns\t-4.06457\n▁Ein\t-4.11399\n,\t-4.20405\n▁the\t-4.35217\n▁und\t-4.5704\n▁mit\t-4.57911\n▁auf\t-4.58144\n▁on\t-4.65674\nn\t-4.67038\n▁Mann\t-4.70521\n▁is\t-4.73988\n▁man\t-4.75331\n▁and\t-4.76404\n▁\t-4.76512\ning\t-4.8072\n▁of\t-4.83344\n▁einer\t-4.86421\n▁with\t-4.93426\n▁Eine\t-4.98902\n▁ein\t-5.126\n","output_type":"stream"}]},{"cell_type":"markdown","source":"As we can see, the vocabulary consists of four special tokens (`<pad>` for padding, `<s>` for beginning of sentence (BOS), `</s>` for end of sentence (EOS), `<unk>` for unknown) and a mixture of German and English words and subwords. In order to ensure reversability, word boundaries are encoded with a special unicode character \"▁\" (U+2581).","metadata":{"execution":{"iopub.status.busy":"2023-09-12T22:55:47.605875Z","iopub.execute_input":"2023-09-12T22:55:47.608994Z","iopub.status.idle":"2023-09-12T22:55:47.622022Z","shell.execute_reply.started":"2023-09-12T22:55:47.608941Z","shell.execute_reply":"2023-09-12T22:55:47.61968Z"}}},{"cell_type":"markdown","source":"To use the vocabulary, we first need to load it from the binary file produced above.","metadata":{"execution":{"iopub.status.busy":"2023-09-12T22:56:07.98859Z","iopub.execute_input":"2023-09-12T22:56:07.989008Z","iopub.status.idle":"2023-09-12T22:56:07.996503Z","shell.execute_reply.started":"2023-09-12T22:56:07.988975Z","shell.execute_reply":"2023-09-12T22:56:07.994868Z"}}},{"cell_type":"code","source":"vocab = sentencepiece.SentencePieceProcessor()\nvocab.Load(\"multi30k.model\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:31:50.133137Z","iopub.execute_input":"2023-09-29T04:31:50.133514Z","iopub.status.idle":"2023-09-29T04:31:50.161558Z","shell.execute_reply.started":"2023-09-29T04:31:50.133481Z","shell.execute_reply":"2023-09-29T04:31:50.160543Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"The vocabulary object includes a number of methods for working with full sequences or individual pieces. We explore the most relevant ones below. A complete interface can be found on [GitHub](https://github.com/google/sentencepiece/tree/master/python#usage) for reference.","metadata":{}},{"cell_type":"code","source":"print(\"Vocabulary size:\", vocab.GetPieceSize())\nprint()\n\nfor example in training_data[:3]:\n  sentence = example.trg\n  pieces = vocab.EncodeAsPieces(sentence)\n  indices = vocab.EncodeAsIds(sentence)\n  print(sentence)\n  print(pieces)\n  print(vocab.DecodePieces(pieces))\n  print(indices)\n  print(vocab.DecodeIds(indices))\n  print()\n\npiece = vocab.EncodeAsPieces(\"the\")[0]\nindex = vocab.PieceToId(piece)\nprint(piece)\nprint(index)\nprint(vocab.IdToPiece(index))","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:31:51.129244Z","iopub.execute_input":"2023-09-29T04:31:51.129590Z","iopub.status.idle":"2023-09-29T04:31:51.137867Z","shell.execute_reply.started":"2023-09-29T04:31:51.129563Z","shell.execute_reply":"2023-09-29T04:31:51.136768Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Vocabulary size: 8000\n\nTwo young, White males are outside near many bushes.\n['▁Two', '▁young', ',', '▁White', '▁males', '▁are', '▁outside', '▁near', '▁many', '▁bushes', '.']\nTwo young, White males are outside near many bushes.\n[42, 54, 12, 2889, 2225, 36, 127, 173, 815, 3513, 4]\nTwo young, White males are outside near many bushes.\n\nSeveral men in hard hats are operating a giant pulley system.\n['▁Se', 'veral', '▁men', '▁in', '▁hard', '▁hats', '▁are', '▁operating', '▁a', '▁g', 'iant', '▁pull', 'e', 'y', '▁s', 'y', 'ste', 'm', '.']\nSeveral men in hard hats are operating a giant pulley system.\n[298, 240, 73, 6, 712, 730, 36, 3106, 5, 631, 1679, 583, 32, 96, 552, 96, 1076, 7, 4]\nSeveral men in hard hats are operating a giant pulley system.\n\nA little girl climbing into a wooden playhouse.\n['▁A', '▁little', '▁girl', '▁climbing', '▁in', 'to', '▁a', '▁wooden', '▁play', 'house', '.']\nA little girl climbing into a wooden playhouse.\n[9, 132, 66, 500, 6, 112, 5, 542, 245, 4599, 4]\nA little girl climbing into a wooden playhouse.\n\n▁the\n13\n▁the\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We define some constants here for the first three special tokens that you may find useful in the following sections.","metadata":{}},{"cell_type":"code","source":"pad_id = vocab.PieceToId(\"<pad>\")\nbos_id = vocab.PieceToId(\"<s>\")\neos_id = vocab.PieceToId(\"</s>\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:31:52.111206Z","iopub.execute_input":"2023-09-29T04:31:52.111549Z","iopub.status.idle":"2023-09-29T04:31:52.117033Z","shell.execute_reply.started":"2023-09-29T04:31:52.111520Z","shell.execute_reply":"2023-09-29T04:31:52.115835Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Note that these tokens will be stripped from the output when converting from word pieces to text. This may be helpful when implementing greedy search and beam search.","metadata":{}},{"cell_type":"code","source":"sentence = training_data[0].trg\nindices = vocab.EncodeAsIds(sentence)\nindices_augmented = [bos_id] + indices + [eos_id, pad_id, pad_id, pad_id]\nprint(vocab.DecodeIds(indices))\nprint(vocab.DecodeIds(indices_augmented))\nprint(vocab.DecodeIds(indices) == vocab.DecodeIds(indices_augmented))","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:32:18.231904Z","iopub.execute_input":"2023-09-29T04:32:18.232246Z","iopub.status.idle":"2023-09-29T04:32:18.238691Z","shell.execute_reply.started":"2023-09-29T04:32:18.232219Z","shell.execute_reply":"2023-09-29T04:32:18.237566Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Two young, White males are outside near many bushes.\nTwo young, White males are outside near many bushes.\nTrue\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Baseline sequence-to-sequence model","metadata":{}},{"cell_type":"markdown","source":"With our data and vocabulary loaded, we're now ready to build a baseline sequence-to-sequence model.  Later on we'll add an attention mechanism to the model.","metadata":{}},{"cell_type":"markdown","source":"Let's begin by defining a batch iterator for the training data. Given a dataset and a batch size, it will iterate over the dataset and yield pairs of tensors containing the subword indices for the source and target sentences in the batch, respectively.  Fill in `make_batch` below.","metadata":{}},{"cell_type":"code","source":"def make_batch(sentences):\n  \"\"\"Convert a list of sentences into a batch of subword indices.\n\n  Args:\n    sentences: A list of sentences, each of which is a string.\n\n  Returns:\n    A LongTensor of size (max_sequence_length, batch_size) containing the\n    subword indices for the sentences, where max_sequence_length is the length\n    of the longest sentence as encoded by the subword vocabulary and batch_size\n    is the number of sentences in the batch. A beginning-of-sentence token\n    should be included before each sequence, and an end-of-sentence token should\n    be included after each sequence. Empty slots at the end of shorter sequences\n    should be filled with padding tokens. The tensor should be located on the\n    device defined at the beginning of the notebook.\n  \"\"\"\n\n  # Implementation tip: You can use the nn.utils.rnn.pad_sequence utility\n  # function to combine a list of variable-length sequences with padding.\n\n  # YOUR CODE HERE\n  encoded_sentences = [[bos_id] + vocab.EncodeAsIds(sentence) + [eos_id] for sentence in sentences]\n\n  # Pad sequences with PAD tokens\n  # We use the nn.utils.rnn.pad_sequence utility function for this\n  padded_sentences = nn.utils.rnn.pad_sequence([torch.LongTensor(s) for s in encoded_sentences], \n                                                batch_first=False, \n                                                padding_value=pad_id)\n\n  # The tensor is already on the CPU by default. If you defined a device at the beginning of the notebook,\n  # you can move the tensor to that device like this: padded_sentences = padded_sentences.to(device)\n\n  return padded_sentences\n\n  # BEGIN SOLUTION\n\n  # END SOLUTION\n\ndef make_batch_iterator(dataset, batch_size, shuffle=False):\n  \"\"\"Make a batch iterator that yields source-target pairs.\n\n  Args:\n    dataset: A torchtext dataset object.\n    batch_size: An integer batch size.\n    shuffle: A boolean indicating whether to shuffle the examples.\n\n  Yields:\n    Pairs of tensors constructed by calling the make_batch function on the\n    source and target sentences in the current group of examples. The max\n    sequence length can differ between the source and target tensor, but the\n    batch size will be the same. The final batch may be smaller than the given\n    batch size.\n  \"\"\"\n\n  examples = list(dataset)\n  if shuffle:\n    random.shuffle(examples)\n\n  for start_index in range(0, len(examples), batch_size):\n    example_batch = examples[start_index:start_index + batch_size]\n    source_sentences = [example.src for example in example_batch]\n    target_sentences = [example.trg for example in example_batch]\n    \n    yield make_batch(source_sentences), make_batch(target_sentences)\n\ntest_batch = make_batch([\"a test input\", \"a longer input than the first\"])\nprint(\"Example batch tensor:\")\nprint(test_batch)\nassert test_batch.shape[1] == 2\nassert test_batch[0, 0] == bos_id\nassert test_batch[0, 1] == bos_id\nassert test_batch[-1, 0] == pad_id\nassert test_batch[-1, 1] == eos_id","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:32:21.504758Z","iopub.execute_input":"2023-09-29T04:32:21.505162Z","iopub.status.idle":"2023-09-29T04:32:21.519739Z","shell.execute_reply.started":"2023-09-29T04:32:21.505129Z","shell.execute_reply":"2023-09-29T04:32:21.518570Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Example batch tensor:\ntensor([[   1,    1],\n        [   5,    5],\n        [3966,  354],\n        [   6,   60],\n        [ 236,    6],\n        [ 698,  236],\n        [   2,  698],\n        [   0, 5285],\n        [   0,   13],\n        [   0, 3759],\n        [   0,    2]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we will define the model itself. It should consist of a bidirectional LSTM encoder that encodes the input sentence into a fixed-size representation, and an LSTM decoder that uses this representation to produce the output sentence.","metadata":{}},{"cell_type":"code","source":"vocab_size = vocab.GetPieceSize()\n\nclass Seq2seqBaseline(nn.Module):\n    def __init__(self, hidden_size=256, num_layers=2, dropout=0.1):\n        super(Seq2seqBaseline, self).__init__()\n\n        # Embedding layers\n        self.encoder_embedding = nn.Embedding(vocab_size, hidden_size)\n        self.decoder_embedding = nn.Embedding(vocab_size, hidden_size)\n\n        # LSTM layers\n        self.encoder_lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout, bidirectional=True)\n        self.decoder_lstm = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout)\n\n        # Output layer\n        self.output_layer = nn.Linear(hidden_size, vocab_size)\n\n        # Other hyperparameters\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n\n    def encode(self, source):\n        source = source.to(self.encoder_embedding.weight.device)\n        embedded = self.encoder_embedding(source)\n        lengths = (source != pad_id).sum(dim=0)  # sum over the sequence length dimension\n\n        # Use pack_padded_sequence to handle variable-length sequences\n        packed = nn.utils.rnn.pack_padded_sequence(embedded, lengths.cpu(), enforce_sorted=False)\n        packed_output, (hidden, cell) = self.encoder_lstm(packed)\n\n        # Unpack the output back to the padded sequence format\n        encoder_output, _ = nn.utils.rnn.pad_packed_sequence(packed_output)\n        # ensure output, source, and mask are on the same device\n        encoder_output = encoder_output.to(self.encoder_embedding.weight.device)\n\n        # encoder_mask should have the same first two dimensions as source\n        encoder_mask = (source == pad_id)\n\n        # Correctly reshaping and reducing bidirectional states\n        hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size).sum(dim=1)\n        cell = cell.view(self.num_layers, 2, -1, self.hidden_size).sum(dim=1)\n#         assert(encoder_output.shape[0] == encoder_mask.shape[0])\n#         assert(encoder_output.shape[1] == encoder_mask.shape[1])\n        return encoder_output, encoder_mask, (hidden, cell)\n\n\n    \n    \n\n    def decode(self, decoder_input, initial_hidden, encoder_output, encoder_mask):\n        decoder_input = decoder_input.to(self.decoder_embedding.weight.device)  # Ensure decoder_input is on the same device as the model\n        embedded = self.decoder_embedding(decoder_input)\n        output, (hidden, cell) = self.decoder_lstm(embedded, initial_hidden)\n        logits = self.output_layer(output)\n\n        return logits, (hidden, cell), None\n\n    def compute_loss(self, source, target):\n    # Ensure source and target are on the same device as the model\n        source = source.to(self.encoder_embedding.weight.device)\n        target = target.to(self.decoder_embedding.weight.device)\n\n        encoder_output, encoder_mask, encoder_hidden = self.encode(source)\n        decoder_input = target[:-1]\n        decoder_target = target[1:]\n        logits, _, _ = self.decode(decoder_input, encoder_hidden, encoder_output, encoder_mask)\n        loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), decoder_target.view(-1), ignore_index=pad_id)\n\n        return loss","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:32:22.858969Z","iopub.execute_input":"2023-09-29T04:32:22.859317Z","iopub.status.idle":"2023-09-29T04:32:22.873218Z","shell.execute_reply.started":"2023-09-29T04:32:22.859289Z","shell.execute_reply":"2023-09-29T04:32:22.871919Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"We define the following functions for training.  This code will run as provided, but you are welcome to modify the training loop to adjust the optimizer settings, add learning rate decay, etc.\n","metadata":{}},{"cell_type":"code","source":"def train(model, num_epochs, batch_size, model_file):\n  \"\"\"Train the model and save its best checkpoint.\n\n  Model performance across epochs is evaluated using token-level accuracy on the\n  validation set. The best checkpoint obtained during training will be stored on\n  disk and loaded back into the model at the end of training.\n  \"\"\"\n  optimizer = torch.optim.Adam(model.parameters())\n  best_accuracy = 0.0\n  for epoch in tqdm.notebook.trange(num_epochs, desc=\"training\", unit=\"epoch\"):\n    with tqdm.notebook.tqdm(\n        make_batch_iterator(training_data, batch_size, shuffle=True),\n        desc=\"epoch {}\".format(epoch + 1),\n        unit=\"batch\",\n        total=math.ceil(len(training_data) / batch_size)) as batch_iterator:\n      model.train()\n      total_loss = 0.0\n      for i, (source, target) in enumerate(batch_iterator, start=1):\n        optimizer.zero_grad()\n        loss = model.compute_loss(source, target)\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n        batch_iterator.set_postfix(mean_loss=total_loss / i)\n      validation_perplexity, validation_accuracy = evaluate_next_token(\n          model, validation_data)\n      batch_iterator.set_postfix(\n          mean_loss=total_loss / i,\n          validation_perplexity=validation_perplexity,\n          validation_token_accuracy=validation_accuracy)\n      if validation_accuracy > best_accuracy:\n        print(\n            \"Obtained a new best validation accuracy of {:.2f}, saving model \"\n            \"checkpoint to {}...\".format(validation_accuracy, model_file))\n        torch.save(model.state_dict(), model_file)\n        best_accuracy = validation_accuracy\n  print(\"Reloading best model checkpoint from {}...\".format(model_file))\n  model.load_state_dict(torch.load(model_file))\n\ndef evaluate_next_token(model, dataset, batch_size=32):\n    total_cross_entropy = 0\n    total_predictions = 0\n    correct_predictions = 0\n\n    device = next(model.parameters()).device  # Get the device of the model\n\n    with torch.no_grad():\n        for source, target in make_batch_iterator(dataset, batch_size):\n            source, target = source.to(device), target.to(device)  # Move tensors to the same device as the model\n            encoder_output, encoder_mask, encoder_hidden = model.encode(source)\n            decoder_input, decoder_target = target[:-1], target[1:]\n            logits, decoder_hidden, attention_weights = model.decode(decoder_input, encoder_hidden, encoder_output, encoder_mask)\n\n            total_cross_entropy += F.cross_entropy(\n                logits.permute(1, 2, 0), decoder_target.permute(1, 0),\n                ignore_index=pad_id, reduction=\"sum\").item()\n            total_predictions += (decoder_target != pad_id).sum().item()\n            correct_predictions += (logits.argmax(-1) == decoder_target).float().sum().item()\n\n    return total_cross_entropy, correct_predictions / total_predictions\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:32:24.185974Z","iopub.execute_input":"2023-09-29T04:32:24.186309Z","iopub.status.idle":"2023-09-29T04:32:24.199482Z","shell.execute_reply.started":"2023-09-29T04:32:24.186282Z","shell.execute_reply":"2023-09-29T04:32:24.198461Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"We can now train the baseline model.\n\nSince we haven't yet defined a decoding method to output an entire string, we will measure performance for now by computing perplexity and the accuracy of predicting the next token given a gold prefix of the output. A correct implementation should get a validation token accuracy above 55%. The training code will automatically save the model with the highest validation accuracy and reload that checkpoint's parameters at the end of training.","metadata":{}},{"cell_type":"code","source":"# You are welcome to adjust these parameters based on your model implementation.\nnum_epochs = 10\nbatch_size = 16\n\nbaseline_model = Seq2seqBaseline().to(device)\ntrain(baseline_model, num_epochs, batch_size, \"baseline_model.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:52:35.635813Z","iopub.execute_input":"2023-09-29T04:52:35.636195Z","iopub.status.idle":"2023-09-29T04:57:54.738761Z","shell.execute_reply.started":"2023-09-29T04:52:35.636165Z","shell.execute_reply":"2023-09-29T04:57:54.737585Z"},"trusted":true},"execution_count":27,"outputs":[{"output_type":"display_data","data":{"text/plain":"training:   0%|          | 0/10 [00:00<?, ?epoch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1cff0f70dbde423380694017f75fb9ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"epoch 1:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6c7014f72eb4bae8bf05b2652dfe799"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.45, saving model checkpoint to baseline_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 2:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b64160fb7e475daf4a06d15147e6f1"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.49, saving model checkpoint to baseline_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 3:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e73005114d9d422b8ffbcd74cfb95651"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.52, saving model checkpoint to baseline_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 4:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77267f87b49d4f4e87675347b217341d"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.53, saving model checkpoint to baseline_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 5:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28fff67fca2c4e429c1a46305e662a29"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.54, saving model checkpoint to baseline_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 6:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b359e0238fc24e2b8995e9203275e59b"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.55, saving model checkpoint to baseline_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 7:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36f21411af5049348dc36f2023cae5b1"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.55, saving model checkpoint to baseline_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 8:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20a70d6f79a0416fb73cf42dff1e7d0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"epoch 9:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6647832047bc43e7a1d1cc462195f872"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"epoch 10:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caafd926c8dc481cba0d819fe1713cba"}},"metadata":{}},{"name":"stdout","text":"Reloading best model checkpoint from baseline_model.pt...\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Download your baseline model here.** Once you have a model you are happy with, you are encouraged to download it or save it to your Google Drive in case your session disconnects. The best baseline model has been saved to `baseline_model.pt` in the local filesystem. You will need a trained model while implementing inference below and to generate your final predictions. To download session files from Kaggle, please click the data tab on the right side of the screen and expand the `/kaggle/working` folder. You can do a similar process in Google drive.","metadata":{}},{"cell_type":"markdown","source":"For evaluation, we also need to be able to generate entire strings from the model. We'll first define a greedy inference procedure here. Later on, we'll implement beam search.\n\nA correct implementation of greedy decoding should get above 20 BLEU on the validation set.","metadata":{}},{"cell_type":"code","source":"from torch.nn.utils.rnn import pad_sequence\n\ndef predict_greedy(model, sentences, max_length=100):\n    model.eval()\n    device = next(model.parameters()).device\n\n    # Convert sentences to tensor of word indices\n    encoded_sentences = [[bos_id] + vocab.EncodeAsIds(sentence) + [eos_id] for sentence in sentences]\n    source_indices = pad_sequence([torch.LongTensor(s) for s in encoded_sentences], batch_first=False, padding_value=pad_id)\n    # Encode\n    encoder_output, encoder_mask, encoder_hidden = model.encode(source_indices)\n\n    batch_size = source_indices.size(1)\n    decoder_input = torch.tensor([bos_id] * batch_size, dtype=torch.long, device=device).unsqueeze(0)\n    decoder_hidden = encoder_hidden\n\n    decoded_tokens = torch.zeros(max_length, batch_size, dtype=torch.long, device=device)\n    eos_flags = torch.zeros(batch_size, dtype=torch.bool, device=device)\n\n    for t in range(max_length):\n        logits, decoder_hidden, _ = model.decode(decoder_input, decoder_hidden, encoder_output, encoder_mask)\n        logits = logits.squeeze(0)\n        logits[eos_flags, :] += 1e9  # Large positive number to ensure <PAD> tokens\n\n        chosen_token = logits.argmax(dim=-1)\n        decoded_tokens[t] = chosen_token\n        decoder_input = chosen_token.unsqueeze(0)\n\n        eos_flags |= (chosen_token == eos_id)\n        if eos_flags.all():\n            break\n\n    # Convert to sentences\n    predicted_sentences = []\n    for i in range(decoded_tokens.size(1)):\n        token_list = decoded_tokens[:, i].tolist()\n        if eos_id in token_list:\n            token_list = token_list[:token_list.index(eos_id) + 1]  # Truncate at the first <EOS> token\n        sentence = vocab.DecodeIds(token_list)\n        predicted_sentences.append(sentence)\n\n    return predicted_sentences\n\n\n\ndef evaluate(model, dataset, batch_size=64, method=\"greedy\"):\n  assert method in {\"greedy\", \"beam\"}\n  source_sentences = [example.src for example in dataset]\n  target_sentences = [example.trg for example in dataset]\n  model.eval()\n  predictions = []\n  with torch.no_grad():\n    for start_index in range(0, len(source_sentences), batch_size):\n      if method == \"greedy\":\n        prediction_batch = predict_greedy(\n            model, source_sentences[start_index:start_index + batch_size])\n      else:\n        prediction_batch = predict_beam(\n            model, source_sentences[start_index:start_index + batch_size])\n        prediction_batch = [candidates[0] for candidates in prediction_batch]\n      predictions.extend(prediction_batch)\n  return sacrebleu.corpus_bleu(predictions, [target_sentences]).score\n\nprint(\"Baseline model validation BLEU using greedy search:\",\n      evaluate(baseline_model, validation_data))","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:58:31.412489Z","iopub.execute_input":"2023-09-29T04:58:31.412831Z","iopub.status.idle":"2023-09-29T04:58:32.280742Z","shell.execute_reply.started":"2023-09-29T04:58:31.412802Z","shell.execute_reply":"2023-09-29T04:58:32.279755Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Baseline model validation BLEU using greedy search: 20.176537701716683\n","output_type":"stream"}]},{"cell_type":"code","source":"def show_predictions(model, num_examples=4, include_beam=False):\n  for example in validation_data[:num_examples]:\n    print(\"Input:\")\n    print(\" \", example.src)\n    print(\"Target:\")\n    print(\" \", example.trg)\n    print(\"Greedy prediction:\")\n    print(\" \", predict_greedy(model, [example.src])[0])\n    if include_beam:\n      print(\"Beam predictions:\")\n      for candidate in predict_beam(model, [example.src])[0]:\n        print(\" \", candidate)\n    print()\n\nprint(\"Baseline model sample predictions:\")\nprint()\nshow_predictions(baseline_model)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:58:33.997888Z","iopub.execute_input":"2023-09-29T04:58:33.998287Z","iopub.status.idle":"2023-09-29T04:58:34.063585Z","shell.execute_reply.started":"2023-09-29T04:58:33.998250Z","shell.execute_reply":"2023-09-29T04:58:34.062501Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Baseline model sample predictions:\n\nInput:\n  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\nTarget:\n  A group of men are loading cotton onto a truck\nGreedy prediction:\n  A group of men are trying to be on a wooden fence.\n\nInput:\n  Ein Mann schläft in einem grünen Raum auf einem Sofa.\nTarget:\n  A man sleeping in a green room on a couch.\nGreedy prediction:\n  A man sleeps on a chair in a green chair.\n\nInput:\n  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\nTarget:\n  A boy wearing headphones sits on a woman's shoulders.\nGreedy prediction:\n  A boy with sunglasses on is sitting on a couch.\n\nInput:\n  Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf\nTarget:\n  Two men setting up a blue ice fishing hut on an iced over lake\nGreedy prediction:\n  Two men holding a blue sign on a boat that is being thrown from their way.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Sequence-to-sequence model with attention","metadata":{}},{"cell_type":"markdown","source":"Next, we extend the baseline model to include an attention mechanism in the decoder. This circumvents the need to store all information about the source sentence in a fixed-size representation, and should substantially improve performance and convergence time.\n\nYour implementation should use bilinear attention, where the attention distribution over the encoder outputs $e_1, \\dots, e_n$ given a decoder LSTM output $d$ is obtained via a softmax of the dot products after a suitable projection to get them to the same size: $w_i \\propto \\exp ( d^\\top W e_i )$. The unnormalized attention logits for encoder outputs corresponding to padding tokens should be offset with a large negative value to ensure that the corresponding attention weights are $0$.\n\nAfter computing the attention distribution, take a weighted sum of the projected encoder outputs to obtain the attention context $c = \\sum_i w_i We_i$, and add this to the decoder output $d$ to obtain the final representation to be passed to the vocabulary projection layer.","metadata":{}},{"cell_type":"code","source":"class Seq2seqAttention(Seq2seqBaseline):\n  def __init__(self):\n    super().__init__()\n\n    super(Seq2seqAttention, self).__init__()\n    self.attention_layer = nn.Linear(2*self.hidden_size, self.hidden_size)\n  def decode(self, decoder_input, initial_hidden, encoder_output, encoder_mask):\n    if isinstance(encoder_output, nn.utils.rnn.PackedSequence):\n            encoder_output, _ = nn.utils.rnn.pad_packed_sequence(encoder_output)\n    decoder_input_embedded = self.decoder_embedding(decoder_input)\n    \n    decoder_output, decoder_hidden = self.decoder_lstm(decoder_input_embedded, initial_hidden)\n    \n    encoder_output_projected = self.attention_layer(encoder_output)\n    attention_logits = torch.einsum('ilk,jlk->ilj', decoder_output, encoder_output_projected) \n    \n    attention_logits.masked_fill_(encoder_mask.unsqueeze(0).permute(0, 2, 1), float(-1e9))\n    attention_weights = F.softmax(attention_logits, dim=2)\n    context = torch.einsum('abd,dbc->abc', attention_weights, encoder_output_projected)\n    updated_output = decoder_output + context\n    \n    logits = self.output_layer(updated_output)\n    assert(attention_weights.shape[0] == decoder_input.shape[0])\n    assert(attention_weights.shape[2] == encoder_output.shape[0])\n    assert(attention_weights.shape[1] == encoder_output.shape[1])\n    return logits, decoder_hidden, attention_weights\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:32:34.404889Z","iopub.execute_input":"2023-09-29T04:32:34.405231Z","iopub.status.idle":"2023-09-29T04:32:34.415199Z","shell.execute_reply.started":"2023-09-29T04:32:34.405204Z","shell.execute_reply":"2023-09-29T04:32:34.414066Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"As before, we can train an attention model using the provided training code.\n\nA correct implementation should get a validation token accuracy above 64 and a validation BLEU above 36 with greedy search.","metadata":{}},{"cell_type":"code","source":"# You are welcome to adjust these parameters based on your model implementation.\nnum_epochs = 10\nbatch_size = 16\n\nattention_model = Seq2seqAttention().to(device)\ntrain(attention_model, num_epochs, batch_size, \"attention_model.pt\")\nprint(\"Attention model validation BLEU using greedy search:\",\n      evaluate(attention_model, validation_data))\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:32:35.854370Z","iopub.execute_input":"2023-09-29T04:32:35.855053Z","iopub.status.idle":"2023-09-29T04:38:45.617411Z","shell.execute_reply.started":"2023-09-29T04:32:35.855017Z","shell.execute_reply":"2023-09-29T04:38:45.616335Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"training:   0%|          | 0/10 [00:00<?, ?epoch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"240330a67ea148eabf13a48c18c97501"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"epoch 1:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34435f06033447f3ae6c96c8a146a7ce"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.56, saving model checkpoint to attention_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 2:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6bdc120b4b142ad8716a0f3ec2c4b94"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.61, saving model checkpoint to attention_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 3:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f6e6026a49b45b18a88566279ce86e0"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.63, saving model checkpoint to attention_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 4:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbef3fbca616403da6499c8d6eddc589"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.64, saving model checkpoint to attention_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 5:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1577494d2fd949acad6c5fd248e080a2"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.65, saving model checkpoint to attention_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 6:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e494ee577c840308d668a1e9ceeaed6"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.65, saving model checkpoint to attention_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 7:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b23a9eecdf2948fa9a5edb7ff0890440"}},"metadata":{}},{"name":"stdout","text":"Obtained a new best validation accuracy of 0.65, saving model checkpoint to attention_model.pt...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"epoch 8:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1867097e3a6749ab886fa74a79668c2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"epoch 9:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40fb0fc8522b4711b112a1cb6b617086"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"epoch 10:   0%|          | 0/1813 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cea982db22d74cf8a91db0585d21bdc3"}},"metadata":{}},{"name":"stdout","text":"Reloading best model checkpoint from attention_model.pt...\nAttention model validation BLEU using greedy search: 36.66513423957569\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Download your attention model here.** Once you have a model you are happy with, you are encouraged to download it or save it to your Google Drive in case your session disconnects. The best attention model has been saved to `attention_model.pt` in the local filesystem. You will need a trained model while implementing beam search below and to generate your final predictions.\n","metadata":{}},{"cell_type":"code","source":"print(\"Attention model validation BLEU using greedy search:\",\n      evaluate(attention_model, validation_data))\nprint()\nprint(\"Attention model sample predictions:\")\nprint() \nshow_predictions(attention_model)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:38:52.592493Z","iopub.execute_input":"2023-09-29T04:38:52.592834Z","iopub.status.idle":"2023-09-29T04:38:53.548881Z","shell.execute_reply.started":"2023-09-29T04:38:52.592806Z","shell.execute_reply":"2023-09-29T04:38:53.547815Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Attention model validation BLEU using greedy search: 36.66513423957569\n\nAttention model sample predictions:\n\nInput:\n  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\nTarget:\n  A group of men are loading cotton onto a truck\nGreedy prediction:\n  A group of men loading just on a truck.\n\nInput:\n  Ein Mann schläft in einem grünen Raum auf einem Sofa.\nTarget:\n  A man sleeping in a green room on a couch.\nGreedy prediction:\n  A man sleeping in a green room on a couch.\n\nInput:\n  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\nTarget:\n  A boy wearing headphones sits on a woman's shoulders.\nGreedy prediction:\n  A boy with headphones sitting on his shoulders of a woman.\n\nInput:\n  Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf\nTarget:\n  Two men setting up a blue ice fishing hut on an iced over lake\nGreedy prediction:\n  Two men are building a blue ice cream cones on a ride.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Beam Search","metadata":{}},{"cell_type":"markdown","source":"Now it's time to implement beam search.\n\nSimilar to greedy search, beam search generates one token at a time. However, rather than keeping only the single best hypothesis, we instead keep the top $k$ candidates at each time step. This is accomplished by computing the set of next-token extensions for each item on the beam and finding the top $k$ across all candidates according to total log-probability.\n\nCandidates that are finished should stay on the beam through the end of inference. The search process concludes once all $k$ items on the beam are complete.\n\nWith beam search, you should get an improvement of at least 0.5 BLEU over greedy search, and should reach above 21 BLEU without attention and above 37 BLEU with attention.\n\n**Tips:**\n\n1) A good general strategy when doing complex code like this is to carefully annotate each line with a comment saying what each dimension represents.\n\n2) You should only need one call to topk per step. You do not need to have a topk just over vocabulary first, you can directly go from vocab_size*beam_size to beam_size items.\n\n3) Be sure you are correctly keeping track of which beam item a candidate is selected from and updating the beam states, such as LSTM hidden state, accordingly. A single state from the previous time step may need to be used for multiple new beam items or not at all. This includes all state associated with a beam, including all past tokens output by the beam and any extra tensors such as ones remembering when a beam is finished.\n\n4) Pay attention to how you interleave things when using a single dimension to represent multiple things.  It will make a difference when you start reshaping to separate them out.  It may be easier to start with everything separate, then temporarily combine as needed.\n\n5) For efficiency, we suggest that you implement all beam manipulations using batched PyTorch computations rather than Python for-loops.\n\n6) Once an EOS token has been generated, force the output for that candidate to be padding tokens in all subsequent time steps by adding a large positive number like 1e9 to the appropriate logits. This will ensure that the candidate stays on the beam, as its probability will be very close to 1 and its score will effectively remain the same as when it was first completed.  All other (invalid) token continuations will have extremely low log probability and will not make it onto the beam.\n\n7) While you are encouraged to keep your tensor dimensions constant for simplicity (aside from the sequence length), some special care will need to be taken on the first iteration to ensure that your beam doesn't fill up with k identical copies of the same candidate.\n","metadata":{}},{"cell_type":"code","source":"from torch.nn.functional import log_softmax\nfrom torch.nn.utils.rnn import pad_packed_sequence\n\n\ndef predict_beam(model, sentences, k=5, max_length=100):\n    model.eval()\n    device = next(model.parameters()).device\n    \n    encoded_sentences = [[bos_id] + vocab.EncodeAsIds(sentence) + [eos_id] for sentence in sentences]\n    source_indices = pad_sequence([torch.LongTensor(s) for s in encoded_sentences], batch_first=False, padding_value=pad_id).to(device)\n    encoder_output, encoder_mask, encoder_hidden = model.encode(source_indices) \n    \n    # Unpack PackedSequence\n\n#     print(source_indices.shape)\n#     print(encoder_output_tensor.shape) # (max_source_sequence_length, batch_size, 2 * hidden_size)\n#     print(encoder_mask.shape) # (max_source_sequence_length, batch_size)\n    batch_size = source_indices.size(1)\n    beams = [[] for _ in range(batch_size)]\n    for i in range(batch_size):\n        hidden_state, cell_state = (encoder_hidden[0][:, i:i + 1, :].contiguous(),\n                                    encoder_hidden[1][:, i:i + 1, :].contiguous())\n        beams[i] = [([bos_id], 0.0, (hidden_state, cell_state))]\n    \n    for _ in range(max_length):\n        candidates = [[] for _ in range(batch_size)]\n        \n        for i in range(batch_size):\n            for beam in beams[i]:\n                seq, score, (hidden_state, cell_state) = beam\n                \n                if seq[-1] == eos_id:\n                    # If the last token is eos_id, append the beam to candidates without further extension\n                    candidates[i].append(beam)\n                    continue  # Skip expansion for sequences that have ended\n                \n                # Prepare the input for decoding\n                decoder_input = torch.tensor([seq[-1]], dtype=torch.long, device=device).unsqueeze(0)\n                assert(encoder_output.shape[0] == encoder_mask.shape[0])\n                assert(encoder_output.shape[1] == encoder_mask.shape[1])\n                logits, (next_hidden, next_cell), _ = model.decode(decoder_input, (hidden_state, cell_state), encoder_output[:, i:i + 1, :], encoder_mask[:, i:i + 1])\n                logits = logits.squeeze(0).squeeze(0)\n                \n                # Convert logits to log probabilities\n                log_probs = log_softmax(logits, dim=-1)\n                \n                # Update the candidates\n                topk_values, topk_indices = log_probs.topk(k, dim=-1)\n                for value, index in zip(topk_values, topk_indices):\n                    candidates[i].append((seq + [index.item()], score + value.item(), (next_hidden.contiguous(), next_cell.contiguous())))\n                    \n        # Update beams with top k candidates\n        for i in range(batch_size):\n            beams[i] = sorted(candidates[i], key=lambda x: x[1], reverse=True)[:k]\n\n    # Extract the sequences from the beams\n    predicted_sentences = [[vocab.DecodeIds(seq) for seq, _, _ in beam] for beam in beams]\n    return predicted_sentences\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:58:44.070822Z","iopub.execute_input":"2023-09-29T04:58:44.071223Z","iopub.status.idle":"2023-09-29T04:58:44.084261Z","shell.execute_reply.started":"2023-09-29T04:58:44.071193Z","shell.execute_reply":"2023-09-29T04:58:44.083259Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"print(\"Baseline model validation BLEU using beam search:\",\n      evaluate(baseline_model, validation_data, method=\"beam\"))\nprint()\nprint(\"Baseline model sample predictions:\")\nprint()\nshow_predictions(baseline_model, include_beam=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:58:44.851622Z","iopub.execute_input":"2023-09-29T04:58:44.851999Z","iopub.status.idle":"2023-09-29T04:59:44.715911Z","shell.execute_reply.started":"2023-09-29T04:58:44.851966Z","shell.execute_reply":"2023-09-29T04:59:44.714770Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Baseline model validation BLEU using beam search: 21.027245262272427\n\nBaseline model sample predictions:\n\nInput:\n  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\nTarget:\n  A group of men are loading cotton onto a truck\nGreedy prediction:\n  A group of men are trying to be on a wooden fence.\nBeam predictions:\n  A group of men trying to catch a rope.\n  A group of men trying to climb on a rope.\n  A group of men trying to climb on a grassy field.\n  A group of men trying to climb on a wooden fence.\n  A group of men trying to be on a wooden fence.\n\nInput:\n  Ein Mann schläft in einem grünen Raum auf einem Sofa.\nTarget:\n  A man sleeping in a green room on a couch.\nGreedy prediction:\n  A man sleeps on a chair in a green chair.\nBeam predictions:\n  A man is sleeping on a chair in a green chair.\n  A man is sleeping on a chair in a blue chair.\n  A man is sleeping on a couch in a green chair.\n  A man is sleeping on a chair in a green room.\n  A man is sleeping on a couch in a blue chair.\n\nInput:\n  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\nTarget:\n  A boy wearing headphones sits on a woman's shoulders.\nGreedy prediction:\n  A boy with sunglasses on is sitting on a couch.\nBeam predictions:\n  A boy with sunglasses is sitting on a chair with a woman.\n  A boy with sunglasses is sitting on a couch with a woman.\n  A boy wearing sunglasses is sitting on a chair with a woman.\n  A boy is sitting on the floor with a child on a black umbrella.\n  A boy is sitting on the floor with a woman on a black umbrella.\n\nInput:\n  Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf\nTarget:\n  Two men setting up a blue ice fishing hut on an iced over lake\nGreedy prediction:\n  Two men holding a blue sign on a boat that is being thrown from their way.\nBeam predictions:\n  Two men pulling a blue boat on a boat that has just thrown.\n  Two men pulling a blue boat on a dock that has just thrown.\n  Two men pulling a blue boat at a boat that has just thrown.\n  Two men pulling a blue boat on a boat that has just thrown them.\n  Two men pulling a blue boat on a boat that has just thrown their way.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Attention model validation BLEU using beam search:\",\n      evaluate(attention_model, validation_data, method=\"beam\"))\nprint()\nprint(\"Attention model sample predictions:\")\nprint()\nshow_predictions(attention_model, include_beam=True)","metadata":{"execution":{"iopub.status.busy":"2023-09-29T04:59:52.203500Z","iopub.execute_input":"2023-09-29T04:59:52.203876Z","iopub.status.idle":"2023-09-29T05:01:13.179120Z","shell.execute_reply.started":"2023-09-29T04:59:52.203825Z","shell.execute_reply":"2023-09-29T05:01:13.178013Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Attention model validation BLEU using beam search: 37.96384820030504\n\nAttention model sample predictions:\n\nInput:\n  Eine Gruppe von Männern lädt Baumwolle auf einen Lastwagen\nTarget:\n  A group of men are loading cotton onto a truck\nGreedy prediction:\n  A group of men loading just on a truck.\nBeam predictions:\n  A group of men loading on a truck.\n  A group of men loading on a truck\n  A group of men loading just on a truck.\n  A group of men loading on the truck.\n  A group of men loading and loading a truck.\n\nInput:\n  Ein Mann schläft in einem grünen Raum auf einem Sofa.\nTarget:\n  A man sleeping in a green room on a couch.\nGreedy prediction:\n  A man sleeping in a green room on a couch.\nBeam predictions:\n  A man sleeping in a green room on a couch.\n  A man sleeps on a couch in a green room.\n  A man sleeping on a couch in a green room.\n  A man sleeping in a green room on a sofa.\n  A man sleeping in a green room.\n\nInput:\n  Ein Junge mit Kopfhörern sitzt auf den Schultern einer Frau.\nTarget:\n  A boy wearing headphones sits on a woman's shoulders.\nGreedy prediction:\n  A boy with headphones sitting on his shoulders of a woman.\nBeam predictions:\n  A boy with headphones is sitting on the shoulders of a woman.\n  A boy wearing headphones is sitting on the shoulders of a woman.\n  A boy with headphones is sitting on the side of a woman.\n  A young boy wearing headphones is sitting on the shoulders of a woman.\n  A young boy wearing headphones is sitting on the side of a woman.\n\nInput:\n  Zwei Männer bauen eine blaue Eisfischerhütte auf einem zugefrorenen See auf\nTarget:\n  Two men setting up a blue ice fishing hut on an iced over lake\nGreedy prediction:\n  Two men are building a blue ice cream cones on a ride.\nBeam predictions:\n  Two men are building a blue ice cream cones on a ride.\n  Two men are building a blue ice cream box on a ride.\n  Two men are building a blue ice cream box on a rock.\n  Two men are building a blue ice cream cones on top of a dock.\n  Two men are building a blue ice cream cones on top of a rock lake\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Attention visualization: 1-Page Analysis","metadata":{}},{"cell_type":"markdown","source":"Once you have everything working in the sections above, add some code here to visualize the decoder attention learned by the attention model using `matplotlib`.\n\nYou may visualize decoder attention on gold source-target pairs from the validation data. You do not need to run any inference.\n\nFor this section, you will submit a write-up interpreting attention maps generated by your model. Your write-up should be 1-page maximum in length and should be submitted in PDF format. You may use any editor you like, but we recommend using LaTeX and working in an environment like Overleaf. For full credit, your write-up should include:\n\n* A figure with attention map plots for 4 sentence pairs from the validation set (the method `imshow`, or equivalent, will likely be useful here). We encourage you to look through more maps to aid your analysis, but please only include 4 representative plots in the figure.\n* A brief discussion over trends you discover in the plots. Do the maps line up with your intuition, are there any surprising alignments? Are there any many-to-one or many-to-many alignments, or mainly one-to-one? Using a tool like Google Translate on substrings may help give some insight into this.\n\nWhen you submit the file, please name it report.pdf.","metadata":{}},{"cell_type":"code","source":"sample_pairs = validation_data[:4]  # Assuming validation_data is a list of sentence pairs\n\nmodel = attention_model\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_attention_maps(model, sample_pairs):\n    fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n    axs = axs.ravel()\n\n    for i, example in enumerate(sample_pairs):\n        source_sentence = example.src\n        target_sentence = example.trg\n\n        # Convert sentences to tensor of word indices\n        encoded_source = [bos_id] + vocab.EncodeAsIds(source_sentence) + [eos_id]\n        source_indices = torch.LongTensor(encoded_source).unsqueeze(1).to(model.encoder_embedding.weight.device)  # Adjust dimensions and device\n\n        encoded_target = [bos_id] + vocab.EncodeAsIds(target_sentence) + [eos_id]\n        target_indices = torch.LongTensor(encoded_target).unsqueeze(1).to(model.decoder_embedding.weight.device)  # Adjust dimensions and device\n\n        # Encode\n        encoder_output, encoder_mask, encoder_hidden = model.encode(source_indices)\n\n        attention_maps = []\n        decoder_hidden = encoder_hidden\n        for t in range(target_indices.shape[0] - 1):  # -1 to avoid the last <eos> token as input\n            decoder_input = target_indices[t].unsqueeze(0)  # Use target token as input\n            logits, decoder_hidden, attention_weights = model.decode(decoder_input, decoder_hidden, encoder_output, encoder_mask)\n            attention_maps.append(attention_weights.squeeze(0).cpu().detach().numpy())\n\n        attention_maps = np.squeeze(np.stack(attention_maps), axis=1) \n\n        ax = axs[i]\n        cax = ax.matshow(attention_maps, cmap='viridis')\n        ax.set_xticklabels([''] + source_sentence.split(' '), rotation=90)\n        ax.set_yticklabels([''] + target_sentence.split(' '))\n        ax.xaxis.set_major_locator(plt.MultipleLocator(1))\n        ax.yaxis.set_major_locator(plt.MultipleLocator(1))\n        plt.colorbar(cax, ax=ax)\n\n    plt.show()\n\n# Call the plotting function\nplot_attention_maps(model, sample_pairs)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-09-29T05:01:48.862929Z","iopub.execute_input":"2023-09-29T05:01:48.863316Z","iopub.status.idle":"2023-09-29T05:01:50.357116Z","shell.execute_reply.started":"2023-09-29T05:01:48.863288Z","shell.execute_reply":"2023-09-29T05:01:50.356203Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:36: UserWarning: FixedFormatter should only be used together with FixedLocator\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:37: UserWarning: FixedFormatter should only be used together with FixedLocator\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 720x720 with 8 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"\\* Describe your findings in this text cell. \\*\n","metadata":{}},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"markdown","source":"Turn in the following files on Gradescope:\n* hw2a.ipynb (IBM alignment notebook)\n* example_alignments.pkl (generated from part 2a)\n* multi20k_alignments.pkl (generated from part 2a)\n* hw2b.ipynb (this notebook)\n* predictions.json (the predictions file generated by running the cell below)\n* report.pdf\n\nBe sure to check the output of the autograder after it runs.  It should confirm that no files are missing and that the output files have the correct format.\n\nThe code below will generate the required predictions file.  **Note** that it is designed to create the file even if some required elements are missing so that you can submit for partial credit.  If you want full credit, you should check the output to make sure there are no warnings indicating missing portions.","metadata":{}},{"cell_type":"code","source":"# Run this cell to generate the predictions.json file required for submission.\n\ndef get_raw_predictions(model, dataset, method, batch_size=64):\n  assert method in {\"greedy\", \"beam\"}\n  source_sentences = [example.src for example in dataset]\n  target_sentences = [example.trg for example in dataset]\n  model.eval()\n  predictions = []\n  with torch.no_grad():\n    for start_index in range(0, len(source_sentences), batch_size):\n      if method == \"greedy\":\n        prediction_batch = predict_greedy(\n            model, source_sentences[start_index:start_index + batch_size])\n      else:\n        prediction_batch = predict_beam(\n            model, source_sentences[start_index:start_index + batch_size])\n      predictions.extend(prediction_batch)\n  return predictions\n\ndef generate_predictions_file_for_submission(filepath):\n  models = {\"baseline\": baseline_model, \"attention\": attention_model}\n  datasets = {\"validation\": validation_data, \"test\": test_data}\n  methods = [\"greedy\", \"beam\"]\n  predictions = {}\n  for model_name, model in models.items():\n    for dataset_name, dataset in datasets.items():\n      for method in methods:\n        print(\n            \"Getting predictions for {} model on {} set using {} \"\n            \"search...\".format(model_name, dataset_name, method))\n        if model_name not in predictions:\n          predictions[model_name] = {}\n        if dataset_name not in predictions[model_name]:\n          predictions[model_name][dataset_name] = {}\n        try:\n          predictions[model_name][dataset_name][method] = get_raw_predictions(\n              model, dataset, method)\n        except:\n          print(\"!!! WARNING: An exception was raised, setting predictions to None !!!\")\n          predictions[model_name][dataset_name][method] = None\n  print(\"Writing predictions to {}...\".format(filepath))\n  with open(filepath, \"w\") as outfile:\n    json.dump(predictions, outfile, indent=2)\n  print(\"Finished writing predictions to {}.\".format(filepath))\n\ngenerate_predictions_file_for_submission(\"predictions.json\")","metadata":{"execution":{"iopub.status.busy":"2023-09-29T05:03:48.436617Z","iopub.execute_input":"2023-09-29T05:03:48.437014Z","iopub.status.idle":"2023-09-29T05:08:22.787760Z","shell.execute_reply.started":"2023-09-29T05:03:48.436980Z","shell.execute_reply":"2023-09-29T05:08:22.786705Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Getting predictions for baseline model on validation set using greedy search...\nGetting predictions for baseline model on validation set using beam search...\nGetting predictions for baseline model on test set using greedy search...\nGetting predictions for baseline model on test set using beam search...\nGetting predictions for attention model on validation set using greedy search...\nGetting predictions for attention model on validation set using beam search...\nGetting predictions for attention model on test set using greedy search...\nGetting predictions for attention model on test set using beam search...\nWriting predictions to predictions.json...\nFinished writing predictions to predictions.json.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}